{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.csv: A full training dataset with the following attributes:\n",
    "\n",
    "* id: unique id for a news article\n",
    "* title: the title of a news article\n",
    "* author: author of the news article\n",
    "* text: the text of the article; could be incomplete\n",
    "* label: a label that marks the article as potentially unreliable\n",
    "* 1: unreliable\n",
    "* 0: reliable\n",
    "\n",
    "source: https://www.kaggle.com/c/fake-news/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "from wordcloud import WordCloud\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new column that combines all the fields: title, author, and text\n",
    "df['all'] = df['title'] + ' ' + df['author'] + ' ' + df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows where title = NaN\n",
    "df_drop = df.dropna(subset=['all']).reset_index(drop=True)\n",
    "df_drop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using wordcloud to visualize common words for both reliable and unrealible news ###\n",
    "reliable = df_drop[df['label'] == 0]\n",
    "unreliable = df_drop[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to list\n",
    "rel_words = reliable['all'].astype(str).tolist()\n",
    "unrel_words = unreliable['all'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining into one string\n",
    "rel_words_onestring = \" \".join(rel_words)\n",
    "unrel_words_onestring = \" \".join(unrel_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting reliable news\n",
    "plt.figure(figsize=(20,20));\n",
    "plt.imshow(WordCloud().generate(rel_words_onestring));\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting unrealiable news\n",
    "plt.figure(figsize=(20,20));\n",
    "plt.imshow(WordCloud().generate(unrel_words_onestring));\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reliable vs unrealiable split\n",
    "print( 'Unreliable percentage =', round((len(unreliable) / len(df_drop) )*100, 2),\"%\")\n",
    "print( 'Reliable percentage =', round((len(reliable) / len(df_drop) )*100, 2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing reliable vs unrealiable\n",
    "sns.countplot(df['label'], label = \"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new copy of the dataframe\n",
    "df_clean = df_drop.copy()\n",
    "\n",
    "# Convert all characters to lowercase - this may not be necessary if we let \n",
    "# CountVectorizer do it for us, but it doesn't take long enough to worry about.\n",
    "df_clean['all'] = df_clean['all'].str.lower()\n",
    "\n",
    "# removing possesives and contractions\n",
    "df_clean['all'] = df_clean['all'].replace(\"â€™s\",\"\", regex=True)\n",
    "\n",
    "# replacing '\\n' with blank space\n",
    "df_clean['all'] = df_clean['all'].replace('\\n',' ', regex=True)\n",
    "\n",
    "# removing special characters (regex)\n",
    "df_clean['all'] = df_clean['all'].replace('[^A-Za-z0-9\\s]+', '',regex=True)\n",
    "\n",
    "# removing leading and trailing spaces\n",
    "df_clean['all'] = df_clean['all'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create the vectorizer by letting CountVectorizer handle tokenization and\n",
    "# stop-words removal. Note that this will not update the original dataframe,\n",
    "# but will instead create X. \n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), stop_words=stopwords.words('english')).fit(df_clean['all'])\n",
    "\n",
    "X = vectorizer.transform(df_clean['all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_drop['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying Naive Bayes classifier to the training data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_classifier = MultinomialNB()\n",
    "model = NB_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on testing data and getting the model score\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for the Training set\n",
    ".y_predict_train = NB_classifier.predict(X_train)\n",
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix when predicting the Test set results\n",
    "y_predict_test = NB_classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the classification report\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model in the current working directory\n",
    "joblib_file = \"News_ish.pkl\"\n",
    "joblib.dump(model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model from file\n",
    "joblib_file = \"News_ish.pkl\"\n",
    "loaded_model = joblib.load(joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the vectorizer in the current working directory\n",
    "joblib_vector_file = \"vectorizer.pkl\"\n",
    "joblib.dump(vectorizer, joblib_vector_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading vectorizer from file\n",
    "joblib_vector_file = \"vectorizer.pkl\"\n",
    "loaded_vectorizer = joblib.load(joblib_vector_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model score\n",
    "score = loaded_model.score(X_test, y_test)\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# y_predict = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with a new input from StarTribune news website.\n",
    "input_message = str(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Minnesota is reporting 45 new COVID-19 deaths and more than 9,000 coronavirus cases in an unusual release Saturday that covers two days worth of data.\n",
    "The latest figures cap a week when the number of COVID-19 deaths reported by the state each day fluctuated greatly.\n",
    "The Minnesota Department of Health reported 72 deaths for the 24-hour period ending at 4 p.m. on Tuesday, and a record 101 deaths reported for the 24-hour period ending at 4 p.m. Wednesday. For the 48-hour period ending Friday afternoon, the state reported fewer than 50 deaths.\n",
    "Funeral home directors and medical examiners need to file reports within five days of death, according to the Health Department. It's possible they pushed to file reports before Thanksgiving, so they wouldn't have to do so on the holiday weekend, said Kris Ehresmann, the state's director for infectious diseases.\n",
    "It's harder to say why the two-day totals released Saturday for new cases and completed tests were low, Ehresmann said, but the holiday could have influenced decisions about whether people sought testing. Throughout the pandemic, COVID numbers released on Mondays have tended to be lower due to reduced testing and reporting activity on weekends.\n",
    "With the latest figures, Minnesota has now seen 304,023 positive cases, 16,423 hospitalizations and 3,521 deaths since the pandemic arrived here in March.\n",
    "Residents of long-term care and assisted-living facilities accounted for 23 of the newly announced deaths, and 2,378 deaths since the start of the pandemic.\n",
    "The state's two-day count of 9,040 new cases came on a low volume of 36,601 newly completed tests, according to the Star Tribune's coronavirus tracker.\n",
    "Minnesota did not plan to update its dashboard for hospital capacity on Saturday, but the Star Tribune tracker shows 380 new admissions reported over the two-day period. The one-day figures on each of the last three Saturdays were 283, 271 and 201 new admissions.\n",
    "Daily reports of new admissions typically include patients who have entered the hospital at some point over the last several days â€” not just on the most recent day.\n",
    "Numbers released Saturday show health care workers have accounted for 22,292 positive cases â€” up by more than 200 cases from last week. More than 257,000 people who were infected no longer need to be isolated.\n",
    "COVID-19 is a viral respiratory illness caused by a new coronavirus that surfaced late last year. People at greatest risk include those 65 and older, residents of long-term care facilities and those with underlying medical conditions.\n",
    "Those health problems range from lung disease and serious heart conditions to severe obesity and diabetes. People undergoing treatment for failing kidneys also run a greater risk, as do those with cancer and other conditions where treatments suppress immune systems.\n",
    "Most patients with COVID-19 don't need to be hospitalized. Most illnesses involve mild or moderate symptoms; many cases are asymptomatic.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing string punctuation for characters removal\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of characters to be removed. The app.py function includes 'â€”'\n",
    "bad_char = [i for i in string.punctuation]\n",
    "print(bad_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess new text\n",
    "from nltk.corpus import stopwords\n",
    "def text_prepro(message):\n",
    "\n",
    "    message = input_message.lower()\n",
    "    \n",
    "    # removing possesives and contractions\n",
    "    message = message.replace(\"'s\",\"\")\n",
    "    \n",
    "    # removing long dash (not in string.punctuation)\n",
    "    message = message.replace(\"â€”\",\"\")\n",
    "    \n",
    "    # replacing '\\n' with blank space\n",
    "    message = message.replace('\\n',' ')\n",
    "    \n",
    "    # removing special characters (regex)\n",
    "    message_nochar = ''.join((filter(lambda i: i not in bad_char, message)))\n",
    "    \n",
    "    # # removing leading and trailing spaces\n",
    "    message_nospace = message_nochar.strip()\n",
    "    \n",
    "    return message_nospace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new imput preprocessing\n",
    "message_preprocessed = text_prepro(input_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(message_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver list into list type\n",
    "list_test = [message_preprocessed]\n",
    "list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new input classification\n",
    "result = loaded_model.predict(loaded_vectorizer.transform(list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
